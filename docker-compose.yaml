version: '3.8'

services:
  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
    ports:
      - "8000:8000"
    environment:
      - NVIDIA_API_KEY=${NVIDIA_API_KEY}
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - LLM_PROVIDER=${LLM_PROVIDER:-openai}
      - LLM_MODEL=${LLM_MODEL:-gpt-4}
    volumes:
      - ./data:/app/data
      - ./output:/app/output
      - ./models:/app/models
    # GPU support (requires nvidia-docker)
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    restart: unless-stopped
    networks:
      - film-network

  # Optional: GPU-accelerated video processing service
  video-worker:
    image: nvidia/cuda:12.1-base-ubuntu22.04
    volumes:
      - ./data:/data
      - ./output:/output
    command: >
      python -c "
      import torch
      print(f'CUDA available: {torch.cuda.is_available()}')
      if torch.cuda.is_available():
          print(f'GPU: {torch.cuda.get_device_name(0)}')
      "
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    networks:
      - film-network

  # Frontend (static files served via nginx or simple HTTP)
  frontend:
    image: nginx:alpine
    volumes:
      - ./frontend:/usr/share/nginx/html:ro
    ports:
      - "3000:80"
    depends_on:
      - backend
    networks:
      - film-network

networks:
  film-network:
    driver: bridge
